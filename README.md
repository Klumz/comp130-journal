# comp130-journal
Base repository for COMP130 research journal

# Blue Eyes Technology

## Introduction
Humans depend on their ability to perceive, interpret, and react to information. However, could such sensory abilities be implemented and programmed for computers? This is where blue eyes technology comes into play. It is the technology built for analysing, detecting, and reacting accordingly to human emotions used with various electronic gadgets, such as a camera. This technology aims to enable computational machines to absorb and interact with humans based on their emotional information, allowing a natural interaction between the two. The **blue** in **blue eyes** stands for 'bluetooth', which is what enables wireless communication. **Eyes** simply refers to human eyes and their ability to move and gather information. Common Blue Eyes methods involve speech and facial recognition [1].

## The potential
Different types of experimental sensors have been created and proposed to capture emotional states, an example being the **Emotion Mouse** [2]. This device in particular gathers emotional information through touch, which it then reacts to by adapting to the user's emotions to create a better work environment, ultimately aiming to improve user productivity. Another one is the **eyePHONE**, an experimental device that allows users to initiate phone calls by holding eye contact to their eyePHONE, and assuming the receiver agrees to the initiation, they accept it by doing the same, facilitating the use of basic social norms for initiating conversations [3]. Additionally, this device conveys non-verbal availability, as opposed to phone calls, subtly explaining if the caller is currently busy or not. Another notable set of inventions emerge from [paper 4](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.29.1837&rep=rep1&type=pdf) and [5](http://www.cc.gatech.edu/cpl/projects/multicameyetracking/papers/PrePrint.pdf), one focusing on a system that tracks facial features in an MP4/video sequence [4], and the other one focusing on head pose tracking under varying lighting conditions in indoor settings [5]. Such systems and technologies can, for instance, be implemented for disabled individuals. While merely experiments, they can be further developed and improved in order to provide ease of accessibility not only for disabled individuals, but potentially for the business side as well, where user productivity is emphasised. An example of improvement can be taken from [paper 6](http://ijsetr.com/uploads/435621IJSETR4235-180.pdf), which mainly focuses on emotion detection in heavy [image noise](https://en.wikipedia.org/wiki/Image_noise)-filled images [6].

## Conclusion
Blue Eyes technology involves creating systems and devices that work either in business (such as the Emotion Mouse) situations, or primarily to enable ease of access for impaired individuals of varying kinds (such as the eyePHONE). Further research and development are required for this specific field, however has major potential if both existing and new gadgets are programmed, improved and implemented respectfully. 

## Sources:

### [1] [Blue Eyes Technology](http://ieeexplore.ieee.org.ezproxy.falmouth.ac.uk/stamp/stamp.jsp?arnumber=6693995)

This paper introduces a new technique called "Emotion Sensory World", where human emotions are analysed and detected through machine eyes. This is done by a camera taking an image of said eye, processes it using a texture filtering algorithm, then is finally compared with the list of images stores in database. Examples of the emotional distinctions include: 

Happiness: Relaxed and neutral eyes

Anger: Eyebrows are pulled down and inward

Surprise: Eyebrows are raised and curved.

*"Blue Eyes uses sensing technology to identify a user's actions and to extract key information. This information is then analyzed to determine the user's physical, emotional, or informational state, which in turn can be used to make the user more productive by performing expected actions or by providing expected information."*


### [2] [The Emotion Mouse](https://pdfs.semanticscholar.org/91d8/2d479b4469cfc8b2c52005a3f8bbf7d28aae.pdf)

This paper proposes a method for gaining emotional user information through touch, the Emotion Mouse. It provides both a theory and experimentation. The four physiological readings measured during the experiment were heart rate, temperature, GSR (galvanic skin response, basically changes in the electrical resistance of the skin caused by stress, measurable with a galvanometer, e.g. in lie-detector tests.) and somatic movement. Ultimately, the results indicated the theory behind the Emotion Mouse work are fundamentally sound. The physiological measurements correlated to emotions using a correlation model.

*"Our proposed method for gaining user information through touch is via a computer input device, the mouse. From the physiological data obtained from the user, an emotional state may be determined which would then be related to the task the user is currently doing on the computer."* 


### [3] [Establishing Remote Conversations Through Eye Contact With Physical Awareness Proxies](https://static1.squarespace.com/static/519d10a2e4b090350a2b66a0/t/51e5525be4b0ead5a3b6de6c/1373983323680/p948-jabarin.pdf)

This paper presents a mechanism for initiating remote conversations between two users through eye contact, the eyePHONE. It conveys attention using an eye tracking device and a pair of synthetic, proxy eyeballs. The calls are initiated by looking, for a set amount of time, at each other's eyePHONE.

*"Face-to-face interactions provide a rich selection of verbal and non-verbal cues that allow potential interlocutors to negotiate the availability of their attention with great subtlety."*


### [4] [An Active Model For Facial Feature Tracking](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.29.1837&rep=rep1&type=pdf)

This paper provides a system that tracks face and facial features in a video sequence, as well as an experiment showcasing its notable results. It works in real-time, and should be applicable on consumer hardware provided it receives further development.

*"The system uses an initial colour processing step for finding a rough estimate of the position, size, and inplane rotation of the face, followed by a refinement step driven by an Active Model. The latter step refines the previous estimate, and also extracts local animation parameters. The system is able to track the face and some facial features in near real-time, and can compress the result to a bitstream compliant to MPEG-4 Face & Body Animation."*

### [5] [Perceptual User Interfaces using Vision-based Eye Tracking](http://www.cc.gatech.edu/cpl/projects/multicameyetracking/papers/PrePrint.pdf)

This paper presents an experiment on a head pose tracking system with the use of multiple cameras in indoor settings under varying lighting conditions. 

*"A significant amount of past research in human factors suggests that humans are generally interested in what they look at [20, 5]."*


### [6] [Hybrid Technique for Human Face Emotion Detection](https://pdfs.semanticscholar.org/60fe/1016aa59261f9135a036f9e968721fa2930b.pdf)

This paper presents an approach for emotion detection in heavily [noisy images](https://en.wikipedia.org/wiki/Image_noise), focusing on removal of noise from said images. The emotions used are anger, fear, happiness and neutral.

*"There is a vast body of literature on emotions. Recent discoveries suggest that emotions are intricately linked to other functions such as attention, perception, memory, decision making, and learning. This suggests that it may be beneficial for computers to recognize the human userâ€™s emotions and other related cognitive states and expressions."*

